# -*- coding: utf-8 -*-
"""Copy of TCC - Maria Eduarda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18WU8w5HnMDkpCUXzaErIvDJgnalxAcb7
"""
import disarray
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import joblib
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score
import datetime as dt
from scipy.sparse import hstack
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

df_train = pd.read_csv(
    '/home/mariaeduarda/Documents/Personal/TCC/datasets/NSL-KDD/KDDTrain+.txt')
df_test = pd.read_csv(
    '/home/mariaeduarda/Documents/Personal/TCC/datasets/NSL-KDD/KDDTest+.txt')

df_train.drop(columns=df_train.columns[-1], axis=1, inplace=True)
df_test.drop(columns=df_test.columns[-1], axis=1, inplace=True)

columns_list = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',
                'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted',
                'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds',
                'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',
                'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',
                'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',
                'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',
                'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'intrusion_type']

df_train.columns = columns_list
df_test.columns = columns_list

#  COMMENT THE LINES BELOW TO RETURN TO TEST 2
# attacks_train = df_train.query("intrusion_type != 'normal'")['intrusion_type'].unique()
# df_train.replace(to_replace=attacks_train, value='attack', inplace=True)

# attacks_test = df_test.query("intrusion_type != 'normal'")['intrusion_type'].unique()
# df_test.replace(to_replace=attacks_test, value='attack', inplace=True)

print("\nQuantity of data on each class: \n")
print("Train: \n")
print(df_train.drop_duplicates().intrusion_type.value_counts())
print("\nTest: \n")
print(df_test.drop_duplicates().intrusion_type.value_counts())
print("\n\n")

Y_train = df_train['intrusion_type']
X_train = df_train.drop('intrusion_type', axis=1)

Y_test = df_test['intrusion_type']
X_test = df_test.drop('intrusion_type', axis=1)

print('Train data')
print(X_train.shape)
print(Y_train.shape)
print('=' * 20)
print('Test data')
print(X_test.shape)
print(Y_test.shape)

# Vectorizing categorical data using One-hot encoding (protocol, service & flag)

protocol = list(X_train['protocol_type'].values)
protocol = list(set(protocol))
print('Protocol types are:', protocol)
one_hot = CountVectorizer(vocabulary=protocol, binary=True)
train_protocol = one_hot.fit_transform(X_train['protocol_type'].values)
test_protocol = one_hot.transform(X_test['protocol_type'].values)

service = list(X_train['service'].values)
service = list(set(service))
print('Services are:', service)
one_hot = CountVectorizer(vocabulary=service, binary=True)
train_service = one_hot.fit_transform(X_train['service'].values)
test_service = one_hot.transform(X_test['service'].values)

flag = list(X_train['flag'].values)
flag = list(set(flag))
print('Flags are:', flag)
one_hot = CountVectorizer(vocabulary=flag, binary=True)
train_flag = one_hot.fit_transform(X_train['flag'].values)
test_flag = one_hot.transform(X_test['flag'].values)


def feature_scaling(X_train, X_test, feature_name):
    scaler = StandardScaler()

    scaler1 = scaler.fit_transform(X_train[feature_name].values.reshape(-1, 1))
    scaler2 = scaler.transform(X_test[feature_name].values.reshape(-1, 1))

    return scaler1, scaler2


columns_list_merge = ['duration', 'train_protocol', 'train_service', 'train_flag', 'src_bytes', 'dst_bytes', 'land',
                      'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',
                      'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files',
                      'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',
                      'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',
                      'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',
                      'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',
                      'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']

columns_ignore = ['train_protocol', 'train_service', 'train_flag']

x = '1'
y = '2'
for columns in columns_list_merge:
    if columns not in columns_ignore:
        vars()[columns + x], vars()[columns +
             y] = feature_scaling(X_train, X_test, columns)


X_train_1 = hstack((
                   duration1, train_protocol, train_service, train_flag, src_bytes1, dst_bytes1, land1, wrong_fragment1,
                   urgent1, hot1, num_failed_logins1, logged_in1, num_compromised1, root_shell1, su_attempted1,
                   num_root1, num_file_creations1, num_shells1, num_access_files1, is_host_login1, is_guest_login1,
                   count1, srv_count1, serror_rate1, srv_serror_rate1, rerror_rate1, srv_rerror_rate1, same_srv_rate1,
                   diff_srv_rate1, srv_diff_host_rate1, dst_host_count1, dst_host_srv_count1, dst_host_same_srv_rate1,
                   dst_host_diff_srv_rate1, dst_host_same_src_port_rate1, dst_host_srv_diff_host_rate1,
                   dst_host_serror_rate1, dst_host_srv_serror_rate1, dst_host_rerror_rate1, dst_host_srv_rerror_rate1))
X_train_1.shape
X_test_1 = hstack((duration2, test_protocol, test_service, test_flag, src_bytes2, dst_bytes2, land2, wrong_fragment2,
                   urgent2, hot2, num_failed_logins2, logged_in2, num_compromised2, root_shell2, su_attempted2,
                   num_root2, num_file_creations2, num_shells2, num_access_files2, is_host_login2, is_guest_login2,
                   count2, srv_count2, serror_rate2, srv_serror_rate2, rerror_rate2, srv_rerror_rate2, same_srv_rate2,
                   diff_srv_rate2, srv_diff_host_rate2, dst_host_count2, dst_host_srv_count2, dst_host_same_srv_rate2,
                   dst_host_diff_srv_rate2, dst_host_same_src_port_rate2, dst_host_srv_diff_host_rate2,
                   dst_host_serror_rate2, dst_host_srv_serror_rate2, dst_host_rerror_rate2, dst_host_srv_rerror_rate2))
X_test_1.shape

"""### **Functions that will be used for different purposes during the model building stage**"""

# Importing libs


def confusion_matrix_func(Y_test, y_test_pred):
    '''
    This function computes the confusion matrix using Predicted and Actual values and plots a confusion matrix heatmap
    '''
    labels = ['back', 'butter_overflow', 'loadmodule', 'guess_passwd', 'imap', 'ipsweep', 'warezmaster', 'rootkit',
            'multihop', 'neptune', 'nmap', 'normal', 'phf', 'perl', 'pod', 'portsweep', 'ftp_write', 'satan', 'smurf', 'teardrop', 'warezclient', 'land']
    # labels = ['normal', 'attack']
    c_i_drop = ['butter_overflow', 'warezclient', 'imap', 'rootkit']
    C = confusion_matrix(Y_test, y_test_pred, labels=labels)
    # C_DT = confusion_matrix(Y_test_dt, y_test_pred_dt, labels=labels)
    df_nb = pd.DataFrame(C_NB, index=labels, columns=labels)
    df_dt = pd.DataFrame(C_DT, index=labels, columns=labels)
    # cm_df = pd.DataFrame({'Naive Bayes': C_NB, 'Decision Tree': C_DT}, index=labels, columns=labels)
    df_nb.drop(columns=c_i_drop, inplace=True)
    df_nb.drop(index=c_i_drop, inplace=True)
    df_dt.drop(columns=c_i_drop, inplace=True)
    df_dt.drop(index=c_i_drop, inplace=True)

    index = list(set(labels)-set(c_i_drop))
    color = {"Naive Bayes": "red", "Decision Tree": "blue"}

    plt.rcParams.update({'font.size': 13})

    # get metrics for each class
    # cm_df.da.export_metrics()
    # only precision
    df_attacks_precision_nb = df_nb.da.precision*100
    df_attacks_precision_dt = df_dt.da.precision*100
    df_precision = pd.DataFrame(
        {'Naive Bayes': df_attacks_precision_nb, 'Decision Tree': df_attacks_precision_dt}, index=index)
    precision_graph = df_precision.sort_values(
        by='Decision Tree', ascending=False).plot.bar(rot=90, color=color)
    plt.show()

    df_attacks_recall_nb = df_nb.da.recall*100
    df_attacks_recall_dt = df_dt.da.recall*100
    df_recall = pd.DataFrame({'Naive Bayes': df_attacks_recall_nb,
                             'Decision Tree': df_attacks_recall_dt}, index=index)
    recall_graph = df_recall.sort_values(
        by='Decision Tree', ascending=False).plot.bar(rot=90, color=color)
    plt.show()

    df_attacks_f1_score_nb = df_nb.da.f1*100
    df_attacks_f1_score_dt = df_dt.da.f1*100
    df_f1_score = pd.DataFrame(
        {'Naive Bayes': df_attacks_f1_score_nb, 'Decision Tree': df_attacks_f1_score_dt}, index=index)
    f1_score_graph = df_f1_score.sort_values(
        by='Decision Tree', ascending=False).plot.bar(rot=90, color=color)
    plt.show()

    # attacks_precision = cm_df.da.precision*100
    # precision_graph = attacks_precision.plot.bar(rot=0, color='r')
    # plt.show()
    # # only recall
    # attacks_recall = cm_df.da.recall*100
    # recall_graph = attacks_recall.plot.bar(rot=0, color='r')
    # plt.show()
    # only f1
    # attacks_f1_score = cm_df.da.f1*100
    # f1_score_graph = attacks_f1_score.plot.bar(rot=0, color='r')
    # plt.show()

    plt.figure(figsize=(20, 15))
    sns.set(font_scale=1.4)
    sns.heatmap(cm_df, annot=True, annot_kws={
                "size": 12}, fmt='g', xticklabels=labels, yticklabels=labels)
    plt.ylabel('Actual Class')
    plt.xlabel('Predicted Class')

    # plt.show()


def model(model_name, X_train, Y_train, X_test, Y_test):
    '''
    Fits the model on train data and predict the performance on train and test data.
    '''

    # # Naive Bayes
    # hyperparameter = {'var_smoothing': [10 ** x for x in range(-9, 3)]}

    # from sklearn.naive_bayes import GaussianNB
    # nb = GaussianNB()
    # nb_grid = GridSearchCV(nb, param_grid=hyperparameter, cv=5, verbose=1, n_jobs=-1)

    # # Decision Tree
    # hyperparameter = {'max_depth': [5, 10, 20, 50, 100, 500], 'min_samples_split': [5, 10, 100, 500]}
    # from sklearn.tree import DecisionTreeClassifier
    # decision_tree = DecisionTreeClassifier(criterion='gini', splitter='best', class_weight='balanced')
    # decision_tree_grid = GridSearchCV(decision_tree, param_grid=hyperparameter, cv=3, verbose=1, n_jobs=-1)

    # print('Fitting the model and prediction on train data:')
    # start = dt.datetime.now()
    # nb_grid.fit(X_train, Y_train)
    # decision_tree_grid.fit(X_train, Y_train)

    print('Fitting the model and prediction on train data:')
    start = dt.datetime.now()
    model_name.fit(X_train, Y_train)
    y_tr_pred = model_name.predict(X_train)
    print('Completed')
    print('Time taken:',dt.datetime.now()-start)
    print('='*50)
    
    results_tr = dict()
    y_tr_pred = model_name.predict(X_train)    
    results_tr['precision'] = precision_score(Y_train, y_tr_pred, average='weighted')
    results_tr['recall'] = recall_score(Y_train, y_tr_pred, average='weighted')
    results_tr['f1_score'] = f1_score(Y_train, y_tr_pred, average='weighted')
        
    results_test = dict()
    print('Prediction on test data:')
    start = dt.datetime.now()
    y_test_pred = model_name.predict(X_test)
    print('Completed')
    print('Time taken:',dt.datetime.now()-start)
    print('='*50)
    
    print('Performance metrics:')
    print('='*50)
    # print('Confusion Matrix is:')
    # confusion_matrix_func(Y_test, y_test_pred)
    # print('='*50)
    results_test['precision'] = precision_score(Y_test, y_test_pred, average='weighted')
    print('Precision score is:')
    print(precision_score(Y_test, y_test_pred, average='weighted'))
    print('='*50)
    results_test['recall'] = recall_score(Y_test, y_test_pred, average='weighted')
    print('Recall score is:')
    print(recall_score(Y_test, y_test_pred, average='weighted'))
    print('='*50)
    results_test['f1_score'] = f1_score(Y_test, y_test_pred, average='weighted')
    print('F1-score is:')
    print(f1_score(Y_test, y_test_pred, average='weighted'))
    # add the trained  model to the results
    results_test['model'] = model
    
    return results_tr, results_test



def print_grid_search_attributes(model):
    '''
    This function prints all the grid search attributes
    '''

    print('---------------------------')
    print('|      Best Estimator     |')
    print('---------------------------')
    print('\n\t{}\n'.format(model.best_estimator_))
    # parameters that gave best results while performing grid search
    print('---------------------------')
    print('|     Best parameters     |')
    print('---------------------------')
    print('\tParameters of best estimator : \n\n\t{}\n'.format(model.best_params_))
    #  number of cross validation splits
    print('----------------------------------')
    print('|   No of CrossValidation sets   |')
    print('----------------------------------')
    print('\n\tTotal number of cross validation sets: {}\n'.format(model.n_splits_))
    # Average cross validated score of the best estimator, from the Grid Search
    print('---------------------------')
    print('|        Best Score       |')
    print('---------------------------')
    print('\n\tAverage Cross Validate scores of best estimator : \n\n\t{}\n'.format(model.best_score_))


def tpr_fpr_func(Y_tr, Y_pred):
    '''
    This function computes the TPR and FPR scores using the actual and predicetd values.
    '''

    results = dict()
    Y_tr = Y_tr.to_list()
    tp = 0
    fp = 0
    positives = 0
    negatives = 0
    length = len(Y_tr)
    for i in range(len(Y_tr)):
        if Y_tr[i] == 'normal':
            positives += 1
        else:
            negatives += 1

    for i in range(len(Y_pred)):
        if Y_tr[i] == 'normal' and Y_pred[i] == 'normal':
            tp += 1
        elif Y_tr[i] != 'normal' and Y_pred[i] == 'normal':
            fp += 1

    tpr = tp / positives
    fpr = fp / negatives

    results['tp'] = tp
    results['tpr'] = tpr
    results['fp'] = fp
    results['fpr'] = fpr

    return results


"""# **Machine Learning Models**

# Naive Bayes
"""

import resource


def using(point=""):
    usage = resource.getrusage(resource.RUSAGE_SELF)
    return '''%s: usertime=%s systime=%s mem=%s mb
           ''' % (point, usage[0], usage[1],
                  usage[2] / 1024.0)


import tracemalloc


def naive_bayes(X_train_1, Y_train, X_test_1, Y_test):
    print("\n\nStarting Naive Bayes Model\n\n")


    hyperparameter = {'var_smoothing': [10 ** x for x in range(-9, 3)]}

    from sklearn.naive_bayes import GaussianNB
    nb = GaussianNB()
    nb_grid = GridSearchCV(nb, param_grid=hyperparameter, cv=5, verbose=1, n_jobs=-1)
    nb_grid_results_train, nb_grid_results_test = model(nb_grid, X_train_1.toarray(), Y_train, X_test_1.toarray(),
                                                        Y_test)

    print_grid_search_attributes(nb_grid)

    nb_gs = nb_grid.best_estimator_
    y_tr_pred = nb_gs.predict(X_train_1.toarray())
    y_test_pred = nb_gs.predict(X_test_1.toarray())
    tpr_fpr_train = tpr_fpr_func(Y_train, y_tr_pred)
    tpr_fpr_test = tpr_fpr_func(Y_test, y_test_pred)

    print('\nTrain results: ')
    print('-----------')
    print(tpr_fpr_train)
    print('\n')
    print(nb_grid_results_train)

    print('\nTest results: ')
    print('-----------')
    print(tpr_fpr_test)
    print('\n')
    print(nb_grid_results_test)

    
start_time = dt.datetime.now()
start_time_naive_bayes = dt.datetime.now()

tracemalloc.start()
print(using("before"))
naive_bayes(X_train_1, Y_train, X_test_1, Y_test)
print(using("after"))

print("\n -------------------- \n")
print("Total Naive Bayes execution time: " + str(dt.datetime.now() - start_time_naive_bayes))
print("\n -------------------- \n")

current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage is {current:0.2f}MB; Peak was {peak:0.2f}MB")
print(f"Current memory usage is {current / (1024 * 1024)}MB; Peak was {peak / (10 ** 6)}MB")
tracemalloc.stop()

"""### **Decision Tree Classifier**"""

tracemalloc.start()


def decision_tree(X_train_1, X_test_1, Y_train, Y_test):
    print("\n\nStarting Decision Tree Model\n\n")

    hyperparameter = {'max_depth': [5, 10, 20, 50, 100, 500], 'min_samples_split': [5, 10, 100, 500]}
    from sklearn.tree import DecisionTreeClassifier
    decision_tree = DecisionTreeClassifier(criterion='gini', splitter='best', class_weight='balanced')
    decision_tree_grid = GridSearchCV(decision_tree, param_grid=hyperparameter, cv=3, verbose=1, n_jobs=-1)
    dt_grid_results_train, dt_grid_results_test = model(decision_tree_grid, X_train_1.toarray(), Y_train,
                                                        X_test_1.toarray(), Y_test)

    print_grid_search_attributes(decision_tree_grid)

    dt_gs = decision_tree_grid.best_estimator_
    y_tr_pred = dt_gs.predict(X_train_1.toarray())
    y_test_pred = dt_gs.predict(X_test_1.toarray())
    tpr_fpr_train = tpr_fpr_func(Y_train, y_tr_pred)
    tpr_fpr_test = tpr_fpr_func(Y_test, y_test_pred)

    print('\nTrain results: ')
    print('-----------')
    print(tpr_fpr_train)
    print('\n')
    print(dt_grid_results_train)

    print('\nTest results: ')
    print('-----------')
    print(tpr_fpr_test)
    print('\n')
    print(dt_grid_results_test)


start_time_decision_tree = dt.datetime.now()

tracemalloc.start()
print(using("before"))
decision_tree(X_train_1, X_test_1, Y_train, Y_test)
print(using("after"))

print("\n -------------------- \n")
print("Total Decision Tree execution time: " + str(dt.datetime.now() - start_time_decision_tree))
print("\n -------------------- \n")

print("\n -------------------- \n")
print("Total execution time: " + str(dt.datetime.now() - start_time))
print("\n -------------------- \n")

current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage is {current:0.2f}MB; Peak was {peak:0.2f}MB")
print(f"Current memory usage is {current / 10 ** 6}MB; Peak was {peak / 10 ** 6}MB")
tracemalloc.stop()
